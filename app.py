import os
import streamlit as st
from rag import RAGIndex
from models import LLMProvider
from prompts import SYSTEM_PROMPT
from web_ingest import load_vendor_urls, fetch_vendor_docs

st.set_page_config(page_title="React Native Course Assistant", page_icon="ğŸ“±", layout="wide")

# Sidebar
with st.sidebar:
    st.title("âš™ï¸ Cáº¥u hÃ¬nh")
    provider = os.getenv("PROVIDER", "github").lower()
    provider = st.selectbox("Provider", ["github", "google"], index=0 if provider=="github" else 1)
    use_rag = st.checkbox("DÃ¹ng RAG (trÃ­ch tÃ i liá»‡u)", value=True)
    use_local_docs = st.checkbox("DÃ¹ng tÃ i liá»‡u ná»™i bá»™ (data/)", value=True)
    use_vendor_docs = st.checkbox("DÃ¹ng nguá»“n vendor (sources.yaml)", value=True)
    top_k = st.slider("Sá»‘ Ä‘oáº¡n trÃ­ch dáº«n (k)", 1, 8, 4)
    temperature = st.slider("Nhiá»‡t Ä‘á»™ (creativity)", 0.0, 1.0, 0.3)
    st.markdown("---")
    st.caption("Quáº£n lÃ½ API keys trong Streamlit Secrets. KhÃ´ng commit secrets lÃªn GitHub.")

# Cache resources
@st.cache_resource(show_spinner=True)
def load_index():
    idx = RAGIndex(data_dir="data")
    idx.build()  # chá»‰ ná»™i bá»™ trÆ°á»›c
    return idx

@st.cache_resource(show_spinner=True)
def load_llm(provider_choice: str):
    os.environ["PROVIDER"] = provider_choice
    return LLMProvider.from_env()

@st.cache_data(show_spinner=True, ttl=60*60*12)  # cache 12h
def get_vendor_docs():
    urls = load_vendor_urls("sources.yaml")
    return fetch_vendor_docs(urls)

st.title("ğŸ“± Trá»£ lÃ½ mÃ´n React Native (Giáº£ng viÃªn & Há»c viÃªn)")
st.caption("Há»i vá» khÃ¡i niá»‡m, best practices, lab/bÃ i táº­p (gá»£i Ã½), quy Ä‘á»‹nh mÃ´n há»c...")

# Session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "index" not in st.session_state:
    with st.spinner("Äang náº¡p tÃ i liá»‡u ná»™i bá»™ vÃ  xÃ¢y dá»±ng chá»‰ má»¥c..."):
        st.session_state.index = load_index()
if "llm" not in st.session_state:
    with st.spinner("Äang khá»Ÿi táº¡o mÃ´ hÃ¬nh..."):
        st.session_state.llm = load_llm(provider)

# Optional: upload tÃ i liá»‡u bá»• sung ngay trong app
uploaded_files = st.file_uploader(
    "Táº£i thÃªm tÃ i liá»‡u (.md/.txt/.pdf) Ä‘á»ƒ tÄƒng cháº¥t lÆ°á»£ng tráº£ lá»i",
    type=["md", "txt", "pdf"], accept_multiple_files=True
)
if uploaded_files:
    added = st.session_state.index.add_uploaded_files(uploaded_files)
    if added:
        st.success(f"ÄÃ£ thÃªm {added} tÃ i liá»‡u táº£i lÃªn vÃ o chá»‰ má»¥c.")

# Vendor sync
vendor_docs = []
if use_vendor_docs:
    col1, col2 = st.columns([1,1])
    with col1:
        if st.button("ğŸ”„ Sync nguá»“n vendor"):
            st.cache_data.clear()  # lÃ m má»›i cache vendor docs
            st.experimental_rerun()
    with col2:
        st.caption("Sá»­a URLs trong sources.yaml náº¿u muá»‘n bá»• sung/giáº£m bá»›t nguá»“n.")
    with st.spinner("Äang táº£i nguá»“n vendor..."):
        vendor_docs = get_vendor_docs()
    if vendor_docs:
        st.info(f"ÄÃ£ náº¡p {len(vendor_docs)} trang vendor. Sáº½ dÃ¹ng Ä‘á»ƒ trÃ­ch dáº«n khi RAG báº­t.")

# Káº¿t há»£p nguá»“n theo lá»±a chá»n
st.session_state.index.reset_external_docs()
if use_vendor_docs and vendor_docs:
    st.session_state.index.add_external_docs(vendor_docs)
if not use_local_docs:
    st.session_state.index.disable_local_docs()

# Hiá»ƒn thá»‹ lá»‹ch sá»­ chat + trÃ­ch dáº«n
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if "citations" in msg and msg["citations"]:
            with st.expander("Nguá»“n trÃ­ch dáº«n"):
                for c in msg["citations"]:
                    st.write(f"- {c['source']} (score: {c['score']:.3f})")

question = st.chat_input("Äáº·t cÃ¢u há»i vá» React Native, bÃ i táº­p, lab, yÃªu cáº§u mÃ´n há»c...")

def format_context(chunks):
    parts = []
    for i, ch in enumerate(chunks, 1):
        parts.append(f"[{i}] {ch['text']}\nSource: {ch['source']}")
    return "\n\n".join(parts)

if question:
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("Äang suy nghÄ©..."):
            retrieved = []
            context = ""
            if use_rag:
                retrieved = st.session_state.index.search(question, top_k=top_k)
                context = format_context(retrieved) if retrieved else ""
            answer = st.session_state.llm.generate_answer(
                question=question,
                context=context,
                system_prompt=SYSTEM_PROMPT,
                temperature=temperature
            )
            st.markdown(answer)
            citations = [{"source": r["source"], "score": r["score"]} for r in retrieved] if retrieved else []
            if citations:
                with st.expander("Nguá»“n trÃ­ch dáº«n"):
                    for c in citations:
                        st.write(f"- {c['source']} (score: {c['score']:.3f})")
    st.session_state.messages.append({
        "role": "assistant",
        "content": answer,
        "citations": citations if use_rag else []
    })
